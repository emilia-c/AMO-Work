{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SET-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Merge manual JSON and other json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data (replace with your actual file paths)\n",
    "with open('C:/Users/Emilia/Documents/Uni Helsinki/Year Three/AMO Freelance/assistant task/9 term/raw data/national party included/9term_apas_w_nationalParty.json', 'r', encoding='utf-8') as f1, open('C:/Users/Emilia/Documents/Uni Helsinki/Year Three/AMO Freelance/assistant task/9 term/raw data/national party included/manual additions/mep_assistants_manually_added.json', 'r', encoding='utf-8') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Concatenate the two lists\n",
    "merged_data = data1 + data2\n",
    "\n",
    "# Save the merged data to a new file\n",
    "with open('ALL_9TERM_TO_10TERM_MEPS.json', 'w', encoding='utf-8') as f_out:\n",
    "    json.dump(merged_data, f_out, ensure_ascii=False, indent=4)\n",
    "\n",
    "#print(\"The JSON files have been concatenated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Open Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. READ IN DATA\n",
    "file_path = \"C:/Users/Emilia/Documents/Uni Helsinki/Year Three/AMO Freelance/assistant task/9 term/raw data/final national party merged/ALL_9TERM_TO_10TERM_MEPS.json\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file at {file_path} does not exist.\")\n",
    "\n",
    "# Open and load the JSON data from the file using UTF-8 encoding\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        nine_term_data = json.load(file)\n",
    "except UnicodeDecodeError as e:\n",
    "    raise ValueError(f\"Encoding error: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    raise ValueError(f\"Error decoding JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explore JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique entry categories (keys) in the JSON file:\n",
      "assistants\n",
      "assistants.Accredited assistants\n",
      "assistants.Accredited assistants (grouping)\n",
      "assistants.Local assistants\n",
      "assistants.Local assistants (grouping)\n",
      "assistants.Paying agents\n",
      "assistants.Paying agents (grouping)\n",
      "assistants.Service providers\n",
      "assistants.Trainees\n",
      "country\n",
      "date_scraped\n",
      "group\n",
      "name\n",
      "national_party\n"
     ]
    }
   ],
   "source": [
    "def get_unique_keys(data, prefix=''):\n",
    "    unique_keys = set()\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            full_key = f\"{prefix}.{key}\" if prefix else key\n",
    "            unique_keys.add(full_key)\n",
    "            unique_keys.update(get_unique_keys(value, full_key))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            unique_keys.update(get_unique_keys(item, prefix))\n",
    "\n",
    "    return unique_keys\n",
    "\n",
    "# Load your JSON file (replace 'file.json' with your file path)\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Get all unique keys\n",
    "unique_keys = get_unique_keys(json_data)\n",
    "\n",
    "# Print the unique keys\n",
    "print(\"Unique entry categories (keys) in the JSON file:\")\n",
    "for key in sorted(unique_keys):\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'name' entries: 746\n",
      "Unique 'group' entries: 13\n",
      "Unique 'country' entries: 28\n",
      "None\n",
      "\n",
      "European Conservatives and Reformists Group\n",
      "\n",
      "Non-attached Members\n",
      "\n",
      "The Left group in the European Parliament - GUE/NGL\n",
      "\n",
      "Confederal Group of the European United Left - Nordic Green Left\n",
      "\n",
      "Renew Europe Group (04.02.2020-16.06.2024), Group of the European People's Party (Christian Democrats) (17.06.2024-15.07.2024)\n",
      "\n",
      "Group of the Greens/European Free Alliance\n",
      "\n",
      "Group of the European People's Party (Christian Democrats)\n",
      "\n",
      "Renew Europe Group\n",
      "\n",
      "Group of the Progressive Alliance of Socialists and Democrats in the European Parliament\n",
      "\n",
      "Renew Europe Group (until 12-06-2024), Group of the European People's Party (Christian Democrats)\n",
      "\n",
      "Group of the European United Left - Nordic Green Left\n",
      "\n",
      "Identity and Democracy Group\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize sets for unique entries\n",
    "unique_names = set()\n",
    "unique_groups = set()\n",
    "unique_countries = set()\n",
    "\n",
    "# Iterate through the data and add entries to sets\n",
    "for entry in json_data:\n",
    "    unique_names.add(entry.get('name'))\n",
    "    unique_groups.add(entry.get('group'))\n",
    "    unique_countries.add(entry.get('country'))\n",
    "\n",
    "# Print the count of unique entries\n",
    "print(f\"Unique 'name' entries: {len(unique_names)}\")\n",
    "print(f\"Unique 'group' entries: {len(unique_groups)}\")\n",
    "print(f\"Unique 'country' entries: {len(unique_countries)}\")\n",
    "\n",
    "# print unique groups \n",
    "for group in unique_groups:\n",
    "    print(f'{group}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CLEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Convert json to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. JSON TO PANDAS DATAFRAME\n",
    "assistant_to_details = {}  # Dictionary to track which assistants work for which MEPs and parties\n",
    "\n",
    "# Iterate over each MEP's data\n",
    "for mep in nine_term_data:\n",
    "    mep_name = mep['name']\n",
    "    mep_group = mep['group']\n",
    "    mep_party = mep['national_party']\n",
    "    mep_country = mep['country']\n",
    "    mep_date_scraped = mep.get('date_scraped', None)  # Extract the date scraped\n",
    "\n",
    "    # Check for Accredited assistants\n",
    "    if 'Accredited assistants' in mep['assistants']:\n",
    "        assistants = mep['assistants']['Accredited assistants']\n",
    "        \n",
    "        for assistant in assistants:\n",
    "            if assistant not in assistant_to_details:\n",
    "                assistant_to_details[assistant] = {\n",
    "                    'assistant_type': 'apa',\n",
    "                    'meps': set(),\n",
    "                    'groups': set(),\n",
    "                    'parties': set(),\n",
    "                    'countries': set(),\n",
    "                    'dates_scraped': set()  # Add a set for dates\n",
    "                }\n",
    "            assistant_to_details[assistant]['meps'].add(mep_name)\n",
    "            assistant_to_details[assistant]['groups'].add(mep_group)\n",
    "            assistant_to_details[assistant]['parties'].add(mep_party)\n",
    "            assistant_to_details[assistant]['countries'].add(mep_country)\n",
    "            if mep_date_scraped:  # Add date if it exists\n",
    "                assistant_to_details[assistant]['dates_scraped'].add(mep_date_scraped)\n",
    "\n",
    "    # Check for Accredited assistants (grouping)\n",
    "    if 'Accredited assistants (grouping)' in mep['assistants']:\n",
    "        assistants_grouping = mep['assistants']['Accredited assistants (grouping)']\n",
    "        \n",
    "        for assistant in assistants_grouping:\n",
    "            if assistant not in assistant_to_details:\n",
    "                assistant_to_details[assistant] = {\n",
    "                    'assistant_type': 'apa grouped',\n",
    "                    'meps': set(),\n",
    "                    'groups': set(),\n",
    "                    'parties': set(),\n",
    "                    'countries': set(),\n",
    "                    'dates_scraped': set()  # Add a set for dates\n",
    "                }\n",
    "            else:\n",
    "                # If already exists, change to \"both\" if it's a grouped assistant\n",
    "                if assistant_to_details[assistant]['assistant_type'] == 'apa':\n",
    "                    assistant_to_details[assistant]['assistant_type'] = 'both'\n",
    "                    \n",
    "            assistant_to_details[assistant]['meps'].add(mep_name)\n",
    "            assistant_to_details[assistant]['groups'].add(mep_group)\n",
    "            assistant_to_details[assistant]['parties'].add(mep_party)\n",
    "            assistant_to_details[assistant]['countries'].add(mep_country)\n",
    "            if mep_date_scraped:  # Add date if it exists\n",
    "                assistant_to_details[assistant]['dates_scraped'].add(mep_date_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Explore similar assistant names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fuzzy Matching for Manual Exploration of Names\n",
    "def explore_similar_assistants(assistant_details):\n",
    "    assistants_list = list(assistant_details.keys())\n",
    "    similar_assistants_dict = {}\n",
    "\n",
    "    for assistant in assistants_list:\n",
    "        # Find similar assistants\n",
    "        similar_assistants = process.extract(assistant, assistants_list, limit=None)\n",
    "        # Filter out matches between 92% and 98% (exclude 100%)\n",
    "        similar_assistants = [(a, score) for a, score in similar_assistants if 92 <= score < 100]\n",
    "        \n",
    "        # Store results in the dictionary if there are any matches\n",
    "        if similar_assistants:\n",
    "            similar_assistants_dict[assistant] = similar_assistants\n",
    "            \n",
    "    return similar_assistants_dict\n",
    "\n",
    "# Get similar assistants for merging\n",
    "similar_assistants_for_merging = explore_similar_assistants(assistant_to_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Maria Magdalena GONZALEZ GOZALBO': [('Magdalena GONZALEZ GOZALBO', 95)], 'Josep MERCADAL BAQUERO': [('Josep/Pepe MERCADAL BAQUERO', 95)], 'PAULA SENDIN RODRIGUEZ': [('Paula SENDÍN RODRIGUEZ', 98)], 'Maria Immaculada IBANEZ LANA': [('Maria Inmaculada IBANEZ LANA', 96)], 'Gilles Willy SEGERS': [('Gilles Willy B SEGERS', 95)], 'Magdalena GONZALEZ GOZALBO': [('Maria Magdalena GONZALEZ GOZALBO', 95)], 'Josep/Pepe MERCADAL BAQUERO': [('Josep MERCADAL BAQUERO', 95), ('Pepe MERCADAL BAQUERO', 95)], 'Claudia MARTINEZ MUNOZ': [('CLAUDIA MARTÍNEZ MUÑOZ', 95)], 'MARIA MERCEDES GARCIA MUNOZ': [('MARIA MERCEDES GARCIA MUÑOZ', 98)], 'Maria Inmaculada IBANEZ LANA': [('Maria Immaculada IBANEZ LANA', 96)], 'Gilles Willy B SEGERS': [('Gilles Willy SEGERS', 95)], 'Paula SENDIN RODRIGUEZ': [('Paula SENDÍN RODRIGUEZ', 98)], 'Paula SENDÍN RODRIGUEZ': [('PAULA SENDIN RODRIGUEZ', 98), ('Paula SENDIN RODRIGUEZ', 98)], 'CLAUDIA MARTÍNEZ MUÑOZ': [('Claudia MARTINEZ MUNOZ', 95)], 'MARIA MERCEDES GARCIA MUÑOZ': [('MARIA MERCEDES GARCIA MUNOZ', 98)], 'Pepe MERCADAL BAQUERO': [('Josep/Pepe MERCADAL BAQUERO', 95)], 'Arturo VILLARROYA GONZALEZ': [('ARTURO VILLARROYA GONZÁLEZ', 98), ('Arturo VILLARROYA GONZÁLEZ', 98)], 'ARTURO VILLARROYA GONZÁLEZ': [('Arturo VILLARROYA GONZALEZ', 98)], 'Arturo VILLARROYA GONZÁLEZ': [('Arturo VILLARROYA GONZALEZ', 98)]}\n",
      "Maria Magdalena GONZALEZ GOZALBO\n",
      "\n",
      "Josep MERCADAL BAQUERO\n",
      "\n",
      "PAULA SENDIN RODRIGUEZ\n",
      "\n",
      "Maria Immaculada IBANEZ LANA\n",
      "\n",
      "Gilles Willy SEGERS\n",
      "\n",
      "Magdalena GONZALEZ GOZALBO\n",
      "\n",
      "Josep/Pepe MERCADAL BAQUERO\n",
      "\n",
      "Claudia MARTINEZ MUNOZ\n",
      "\n",
      "MARIA MERCEDES GARCIA MUNOZ\n",
      "\n",
      "Maria Inmaculada IBANEZ LANA\n",
      "\n",
      "Gilles Willy B SEGERS\n",
      "\n",
      "Paula SENDIN RODRIGUEZ\n",
      "\n",
      "Paula SENDÍN RODRIGUEZ\n",
      "\n",
      "CLAUDIA MARTÍNEZ MUÑOZ\n",
      "\n",
      "MARIA MERCEDES GARCIA MUÑOZ\n",
      "\n",
      "Pepe MERCADAL BAQUERO\n",
      "\n",
      "Arturo VILLARROYA GONZALEZ\n",
      "\n",
      "ARTURO VILLARROYA GONZÁLEZ\n",
      "\n",
      "Arturo VILLARROYA GONZÁLEZ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(similar_assistants_for_merging)\n",
    "for assistant in similar_assistants_for_merging: \n",
    "    print(f'{assistant}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Merge assistants with similar names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Merge assistants with high similarity scores\n",
    "merged_assistants = {}\n",
    "\n",
    "# Function to merge details\n",
    "def merge_assistant_details(assistant_names):\n",
    "    merged_details = {\n",
    "        'assistant_type': None,\n",
    "        'meps': set(),\n",
    "        'groups':set(),\n",
    "        'parties': set(),\n",
    "        'countries': set(),\n",
    "        'dates_scraped': set(),\n",
    "    }\n",
    "    \n",
    "    for name in assistant_names:\n",
    "        if name in assistant_to_details:\n",
    "            details = assistant_to_details[name]\n",
    "            merged_details['meps'].update(details['meps'])\n",
    "            merged_details['groups'].update(details['groups'])\n",
    "            merged_details['parties'].update(details['parties'])\n",
    "            merged_details['countries'].update(details['countries'])\n",
    "            merged_details['dates_scraped'].update(details['dates_scraped'])\n",
    "            # Set the assistant_type to the most specific type found (apa > apa grouped > both)\n",
    "            if merged_details['assistant_type'] is None:\n",
    "                merged_details['assistant_type'] = details['assistant_type']\n",
    "            else:\n",
    "                # Determine the type hierarchy\n",
    "                if details['assistant_type'] == 'apa grouped':\n",
    "                    merged_details['assistant_type'] = 'both'\n",
    "                elif merged_details['assistant_type'] == 'apa grouped':\n",
    "                    merged_details['assistant_type'] = 'both'\n",
    "\n",
    "    return merged_details\n",
    "\n",
    "# Handle exact matches by checking lowercase equivalence first\n",
    "lowercase_dict = {}\n",
    "for assistant in assistant_to_details.keys():\n",
    "    lower_name = assistant.lower()\n",
    "    if lower_name not in lowercase_dict:\n",
    "        lowercase_dict[lower_name] = [assistant]\n",
    "    else:\n",
    "        lowercase_dict[lower_name].append(assistant)\n",
    "\n",
    "# Merge exact matches (lowercase)\n",
    "for assistants in lowercase_dict.values():\n",
    "    if len(assistants) > 1:  # Only merge if there are duplicates\n",
    "        merged_details = merge_assistant_details(assistants)\n",
    "        # Keep the name with ASCII characters if available\n",
    "        ascii_names = [name for name in assistants if all(ord(char) < 128 for char in name)]\n",
    "        merged_assistant_name = max(ascii_names, key=lambda x: (x.lower(), x)) if ascii_names else assistants[0]\n",
    "        merged_assistants[merged_assistant_name] = merged_details\n",
    "\n",
    "# Now merge based on fuzzy matches (92% to 98%)\n",
    "for assistant, similar in similar_assistants_for_merging.items():\n",
    "    # Get the set of unique assistants in this group\n",
    "    similar_assistants_set = set([assistant] + [name for name, score in similar])\n",
    "    \n",
    "    # Check for existing lowercase matches to avoid duplicates\n",
    "    merged_details = merge_assistant_details(similar_assistants_set)\n",
    "\n",
    "    # Handle the naming preference with ASCII\n",
    "    ascii_names = [name for name in similar_assistants_set if all(ord(char) < 128 for char in name)]\n",
    "    \n",
    "    if ascii_names:\n",
    "        # Choose the name with ASCII characters to keep\n",
    "        merged_assistant_name = max(ascii_names, key=lambda x: (x.lower(), x))\n",
    "    else:\n",
    "        # Just keep the first one in the set if no ASCII names are found\n",
    "        merged_assistant_name = next(iter(similar_assistants_set))\n",
    "\n",
    "    # If this merged assistant already exists, combine the details\n",
    "    if merged_assistant_name not in merged_assistants:\n",
    "        merged_assistants[merged_assistant_name] = merged_details\n",
    "    else:\n",
    "        existing_details = merged_assistants[merged_assistant_name]\n",
    "        existing_details['meps'].update(merged_details['meps'])\n",
    "        existing_details['groups'].update(merged_details['groups'])\n",
    "        existing_details['parties'].update(merged_details['parties'])\n",
    "        existing_details['countries'].update(merged_details['countries'])\n",
    "        existing_details['dates_scraped'].update(merged_details['dates_scraped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add non-merged assistants to the final output\n",
    "for assistant, details in assistant_to_details.items():\n",
    "    if assistant not in merged_assistants:\n",
    "        merged_assistants[assistant] = details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CONVERT TO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the final DataFrame\n",
    "rows = []\n",
    "for assistant, details in merged_assistants.items():\n",
    "    meps_list = ', '.join(details['meps'])\n",
    "    groups_list = ', '.join(details['meps'])\n",
    "    parties_list = ', '.join(details['parties'])\n",
    "    countries_list = ', '.join(details['countries'])\n",
    "    term = '9'\n",
    "    dates_scraped_list = ', '.join(sorted(details['dates_scraped']))  # Join unique dates, sorted\n",
    "    \n",
    "    rows.append({\n",
    "        'assistant_name': assistant,\n",
    "        'assistant_type': details['assistant_type'],  # Maintain original type\n",
    "        'mep(s)': meps_list,\n",
    "        'mep(s) country': countries_list,\n",
    "        'political_group(s)': groups_list,\n",
    "        'mep(s) national parties': parties_list,\n",
    "        'date_scraped': dates_scraped_list,  # Add the dates scraped here\n",
    "        'term': term\n",
    "    })\n",
    "\n",
    "assistants_9term = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ireland' 'Spain' 'Germany' 'Netherlands' 'Belgium' 'Sweden' 'Poland'\n",
      " 'Romania' 'France' 'Italy' 'Austria' 'Czechia' 'United Kingdom' 'Greece'\n",
      " 'Malta' 'Hungary' 'Greece, Italy' 'Bulgaria' 'Latvia' 'Lithuania'\n",
      " 'Estonia' 'United Kingdom, Romania' 'Denmark' 'Finland, United Kingdom'\n",
      " 'Croatia' 'Portugal' 'Luxembourg' 'United Kingdom, Ireland' 'Cyprus'\n",
      " 'Finland' 'Germany, Austria' 'Slovakia' 'Slovenia' 'Spain, France'\n",
      " 'United Kingdom, France' 'Spain, Greece' 'Greece, Austria, Malta']\n"
     ]
    }
   ],
   "source": [
    "#print(assistants_9term.head())\n",
    "print(assistants_9term['mep(s) country'].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mep_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
