{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from wayback import WaybackClient\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Set-Up Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache management\n",
    "CACHE_FILE = 'wayback_cache_TEST.pkl'\n",
    "def load_cache():\n",
    "    if os.path.exists(CACHE_FILE) and os.path.getsize(CACHE_FILE) > 0:\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            try:\n",
    "                return pickle.load(f)\n",
    "            except EOFError:\n",
    "                return {}\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache):\n",
    "    \"\"\"Save the cache to a file.\"\"\"\n",
    "    with open(CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "cache = load_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MEP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEP data classes\n",
    "class ArchivedMEP:\n",
    "    def __init__(self, url: str):\n",
    "        self.url = url\n",
    "        self.name = None\n",
    "        self.mep_party = None\n",
    "        self.assistants = defaultdict(dict)\n",
    "\n",
    "    def get_mep_data(self, snapshot_url: str):\n",
    "        response = requests.get(snapshot_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Scrape data with streamlined tag checks\n",
    "        self.name = (soup.find('span', class_='sln-member-name') or {}).get('text', '').strip()\n",
    "        self.mep_party = (soup.find('h3', class_='erpl_title-h3 mt-1 sln-political-group-name') or {}).get('text', '').strip()\n",
    "\n",
    "        for section in soup.find_all('div', class_='erpl_type-assistants'):\n",
    "            assistant_type = section.find('h4', class_='erpl_title-h4').text.strip()\n",
    "            for assistant_tag in section.find_all('div', class_='erpl_type-assistants-item'):\n",
    "                assistant_name = assistant_tag.find('span', class_='erpl_assistant').text.strip()\n",
    "                self.assistants[assistant_type].setdefault(assistant_name, []).append(snapshot_url)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"name\": self.name, \"party\": self.mep_party, \"assistants\": self.assistants}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_archived_mep_url(mep_name: str, mep_id: str, snapshot_url: str) -> str:\n",
    "    \"\"\"Construct a URL in the form of a Wayback Machine archived snapshot link.\"\"\"\n",
    "    # Split the name to create the correct path format\n",
    "    names = mep_name.split()\n",
    "    first_names = [name for name in names if not name.isupper()]\n",
    "    last_names = [name for name in names if name.isupper()]\n",
    "    \n",
    "    # Join names with underscores, matching the URL style\n",
    "    first_name_part = '_'.join(first_names)\n",
    "    last_name_part = '_'.join(last_names)\n",
    "    \n",
    "    # Construct the final name part as \"FIRSTNAME_LASTNAME\"\n",
    "    name_path = f\"{first_name_part}_{last_name_part}\" if last_names else first_name_part\n",
    "    \n",
    "    # Append the MEP path to the snapshot URL\n",
    "    mep_path = f\"http://www.europarl.europa.eu/meps/en/{mep_id}/{name_path}/assistants#mep-card-content\"\n",
    "    return f\"{snapshot_url}{mep_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Snapshots from Wayback Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts one snapshot per day from the wayback machine from two dates that have been entered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot retrieval\n",
    "def get_wayback_snapshots(base_url: str, from_date: str, to_date: str) -> list:\n",
    "    \"\"\"Fetch and cache snapshots.\"\"\"\n",
    "    client = WaybackClient()\n",
    "    from_date_dt = datetime.strptime(from_date, \"%Y%m%d\")\n",
    "    to_date_dt = datetime.strptime(to_date, \"%Y%m%d\")\n",
    "    print(f\"Fetching snapshots for {base_url} from {from_date_dt} to {to_date_dt}...\")\n",
    "\n",
    "    try:\n",
    "        results = list(client.search(base_url, from_date=from_date_dt, to_date=to_date_dt))\n",
    "        print(f\"Total snapshots found: {len(results)}\")\n",
    "        snapshots_by_day = {result.timestamp: result.raw_url for result in results}\n",
    "        return [url for _, url in sorted(snapshots_by_day.items())]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching snapshots: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract MEP Archived Links\n",
    "Based on the snapshots I have extraced above, I will now try to recreate the links to the meps home pages based on the full list of MEPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archived_mep_links(from_date: str, to_date: str) -> list:\n",
    "    base_url = \"http://www.europarl.europa.eu/meps/en/full-list\"\n",
    "    snapshot_urls = get_wayback_snapshots(base_url, from_date, to_date)\n",
    "    if not snapshot_urls:\n",
    "        print(\"No snapshots found.\")\n",
    "        return []\n",
    "\n",
    "    # Use the first snapshot URL\n",
    "    snapshot_url = snapshot_urls[0]\n",
    "\n",
    "    response = requests.get(snapshot_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    mep_links = []\n",
    "\n",
    "    # Iterate through MEP links and construct each archived URL\n",
    "    for mep in soup.select('a.ep_content'):\n",
    "        mep_name = mep.find('span', class_='ep_name member-name').text.strip()\n",
    "        mep_id = mep['href'].split('/')[-1]\n",
    "        mep_assistant_url = construct_archived_mep_url(mep_name, mep_id, snapshot_url)\n",
    "        mep_links.append(mep_assistant_url)\n",
    "\n",
    "    return mep_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the Damn Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_archived_meps(url):\n",
    "    \"\"\"Scrape MEP data from the archived assistant page.\"\"\"\n",
    "    mep = ArchivedMEP(url)\n",
    "    mep.get_mep_data(url)  # Use the MEP's own snapshot URL\n",
    "    return mep.to_dict()  # Return dictionary directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching archived MEP links...\n",
      "Fetching snapshots for http://www.europarl.europa.eu/meps/en/full-list from 2023-01-01 00:00:00 to 2023-12-31 00:00:00...\n",
      "Total snapshots found: 21\n",
      "No MEP links found for the given date range.\n"
     ]
    }
   ],
   "source": [
    "# Use existing cache and output files\n",
    "CACHE_FILE = 'wayback_cache.pkl'\n",
    "OUTPUT_FILE = \"mep_assistants_test.json\"\n",
    "\n",
    "# Load existing cache\n",
    "cache = load_cache()\n",
    "\n",
    "def save_data_incrementally(data):\n",
    "    \"\"\"Save data incrementally to the JSON file.\"\"\"\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        with open(OUTPUT_FILE, \"r+\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "            existing_data.append(data)\n",
    "            f.seek(0)\n",
    "            json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([data], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    from_date = \"20230101\"  # Start of 2022\n",
    "    to_date = \"20231231\"    # End of 2022\n",
    "\n",
    "    # Step 1: Retrieve archived MEP links\n",
    "    print(\"Fetching archived MEP links...\")\n",
    "    mep_links = get_archived_mep_links(from_date, to_date)\n",
    "    \n",
    "    if not mep_links:\n",
    "        print(\"No MEP links found for the given date range.\")\n",
    "        return\n",
    "\n",
    "    # Limit for testing\n",
    "    mep_links = mep_links[:5]  \n",
    "\n",
    "    print(\"Scraping MEP assistants data...\")\n",
    "    for mep_url in tqdm(mep_links, desc=\"Scraping MEPs\"):\n",
    "        # Check cache first\n",
    "        if mep_url in cache:\n",
    "            print(f\"Using cached data for {mep_url}\")\n",
    "            mep_data = cache[mep_url]\n",
    "        else:\n",
    "            # Scrape and cache new data\n",
    "            try:\n",
    "                mep_data = scrape_archived_meps(mep_url)\n",
    "                cache[mep_url] = mep_data  # Update cache\n",
    "                save_cache(cache)  # Save cache to file\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {mep_url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save data incrementally to JSON\n",
    "        save_data_incrementally(mep_data)\n",
    "        \n",
    "        # Timeout to avoid overloading the API\n",
    "        time.sleep(5)  # Adjust this delay as needed\n",
    "\n",
    "    print(f\"Data collection completed. Output saved incrementally to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mep_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
